{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Marine Mammal Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNooqY1BO5r5q7ArtmjEVF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sedwardsmarsh/Marine-Mammal-Classifier/blob/master/Marine_Mammal_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9pVArPXhRoy",
        "colab_type": "text"
      },
      "source": [
        "#**Marine Mammal Classifier**\n",
        "\n",
        "\n",
        "*   source for audio data: Watkins Marine Mammal Sound Database, Woods Hole Oceanographic Institution: https://whoicf2.whoi.edu/science/B/whalesounds/index.cfm\n",
        "*   thanks to Todd Hayton for the python tutorial *Scraping by Example - Iterating through Select Items With Mechanize*: http://toddhayton.com/2015/01/09/scraping-by-example-ntu-edu/\n",
        "*   this answer from stack exchange was used as well, thank you: https://stackoverflow.com/questions/5974595/download-all-the-linksrelated-documents-on-a-webpage-using-python\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imRrk3Uan6Ew",
        "colab_type": "text"
      },
      "source": [
        "Before running anything, you need to tell Colab that you are interested in using a GPU. You can do this by clicking on the ‘Runtime’ tab and selecting ‘Change runtime type’. A pop-up window will open up with a drop-down menu. Select ‘GPU’ from the menu and click ‘Save’.\n",
        "\n",
        "# ***make these images a lot smaller***\n",
        "\n",
        "![Click the 'Runtime' tab above and select 'Change runtime type'](https://course.fast.ai/images/colab/03.png)\n",
        "\n",
        "![A pop-up window will open up with a drop-down menu. Select ‘GPU’ from the menu and click ‘Save’.](https://course.fast.ai/images/colab/04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaocxwumhnAn",
        "colab_type": "text"
      },
      "source": [
        "# Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65C_9YIGdmJZ",
        "colab_type": "code",
        "outputId": "4ab5d730-39cc-4f74-cdf1-482c627e7a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My\\ Drive/\"\n",
        "data_dir = root_dir + \"Colab\\ Notebooks/watkins_data/\""
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m13I7lI6maj-",
        "colab_type": "code",
        "outputId": "18c7b0dc-0c49-4126-a060-8f7f7cd74b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# create google drive directory to hold watkins marine mammal data\n",
        "!mkdir {data_dir}"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/My’: Operation not supported\n",
            "mkdir: cannot create directory ‘Drive/Colab’: No such file or directory\n",
            "mkdir: cannot create directory ‘Notebooks/watkins_data/’: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E51ZlZC0hg6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fetch the latest fast.ai version \n",
        "!curl -s https://course.fast.ai/setup/colab | bash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lxV6gqGhsq2",
        "colab_type": "code",
        "outputId": "7a5e9827-c837-447c-8aa6-fe48555c19e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# install the latest SoX version\n",
        "!apt-get install -q sox"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "sox is already the newest version (14.4.2-3ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqpgbffKd87w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "48df903e-296d-4006-e77a-1602d015524a"
      },
      "source": [
        "# install the latest mechanize version\n",
        "!pip install mechanize"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mechanize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/08/77368b47ba2f9e0c03f33902ed2c8e0fa83d15d81dcf7fe102b40778d810/mechanize-0.4.5-py2.py3-none-any.whl (109kB)\n",
            "\r\u001b[K     |███                             | 10kB 28.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 30kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 71kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 92kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: html5lib>=0.999999999 in /usr/local/lib/python3.6/dist-packages (from mechanize) (1.0.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from html5lib>=0.999999999->mechanize) (1.12.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib>=0.999999999->mechanize) (0.5.1)\n",
            "Installing collected packages: mechanize\n",
            "Successfully installed mechanize-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zo5nxIlnQR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# import sys\n",
        "import signal\n",
        "import mechanize \n",
        "from time import sleep\n",
        "\n",
        "URL = 'https://whoicf2.whoi.edu/science/B/whalesounds/index.cfm'\n",
        "DELAY = 5\n",
        "\n",
        "# def sigint(signal, frame):\n",
        "#   sys.stderr.write('Exiting...\\n')\n",
        "#   sys.exit(0)    \n",
        "\n",
        "class WatkinsScraper:\n",
        "    def __init__(self, url=URL, delay=DELAY):\n",
        "        # initilize browser, url, delay and items array\n",
        "        self.br = mechanize.Browser()\n",
        "        self.url = url\n",
        "        self.delay = delay\n",
        "        self.items = []\n",
        "\n",
        "\n",
        "    def scrape(self):\n",
        "        '''\n",
        "        Get the list of items in the first dropdown menu, \"Common name\" \n",
        "        and submit the form for each item. \n",
        "        '''\n",
        "        items = self.get_items()\n",
        "\n",
        "        for item in items:\n",
        "            # Skip invalid/blank item selections\n",
        "            if item.get_labels != \"Select\":\n",
        "                continue\n",
        "\n",
        "            response = self.submit_form(item)\n",
        "            links = self.get_links(filetypes=[\".wav\"])\n",
        "            self.save_item_results(item, response, links)\n",
        "\n",
        "\n",
        "    def get_items(self):\n",
        "        '''\n",
        "        Get the list of items in the first dropdown of the form\n",
        "        '''\n",
        "        self.br.open(self.url)\n",
        "        self.br.select_form('jump1')\n",
        "\n",
        "        # get items from submit tag \n",
        "        items = self.br.form.find_control('getSpeciesCommon').get_items()\n",
        "        return items\n",
        "\n",
        "\n",
        "    def submit_form(self, item):\n",
        "        '''\n",
        "        Submit form using selection item.name and download the audio files\n",
        "        to data_dir\n",
        "        '''\n",
        "        max_tries = 3\n",
        "        num_tries = 0\n",
        "\n",
        "        while num_tries < max_tries:\n",
        "            # loop through each item name from submit tag.\n",
        "            try:\n",
        "                self.br.open(self.url)\n",
        "                self.br.select_form('jump1')\n",
        "                self.br.form['getSpeciesCommon'] = [ item.name ]\n",
        "                self.br.submit()\n",
        "                break\n",
        "            # unless encountering an error.\n",
        "            except (mechanize.HTTPError, mechanize.URLError) as e:\n",
        "                if isinstance(e,mechanize.HTTPError):\n",
        "                    print(e.code)\n",
        "                else:\n",
        "                    print(e.reason.args)\n",
        "\n",
        "            num_tries += 1\n",
        "            time.sleep(num_tries * self.delay)\n",
        "\n",
        "        if num_tries == max_tries:\n",
        "            raise\n",
        "\n",
        "        # return page response from server.\n",
        "        return self.br.response().read()\n",
        "\n",
        "\n",
        "    def get_links(self, filetypes=[]):\n",
        "        '''\n",
        "        Locates the links on a given webpage\n",
        "        '''\n",
        "        # myfiles holds the links of the files we want to download.\n",
        "        myfiles=[]\n",
        "        # iterate through links inside browser on the page.\n",
        "        for link in br.links():\n",
        "            # check if this link has the file extension we want.\n",
        "            for ft in filetypes:\n",
        "                if ft in str(link): \n",
        "                    myfiles.append(link)\n",
        "\n",
        "        return myfiles\n",
        "\n",
        "\n",
        "    def download_link(self, link):\n",
        "        # perhaps you should open in a better way & ensure \n",
        "        # that file doesn't already exist.\n",
        "        f = open(link.text,\"w\") \n",
        "        br.click_link(link)\n",
        "        f.write(br.response().read())\n",
        "        print (\"%s has been downloaded\" % link.text)\n",
        "\n",
        "\n",
        "    def save_item_results(self, item, response, myfiles):\n",
        "        label = ' '.join([label.text for label in item.get_labels()])\n",
        "        label = '-'.join(label.split())\n",
        "        \n",
        "        for link in myfiles:\n",
        "            # throttle so you dont hammer the site\n",
        "            sleep(self.delay) \n",
        "            self.download_link(link)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # signal.signal(signal.SIGINT, sigint)\n",
        "    scraper = WatkinsScraper()\n",
        "    scraper.scrape()\n",
        "    # some_items = scraper.get_items()\n",
        "    # token = scraper.save_item_results(item=some_items[1])\n",
        "    # for x in zip(some_items): \n",
        "    #     print(x)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLK95RZckeea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}